Exercise 3.1:
Skal udregne 5ED4 - 07A4, når disse er unsigned 16-bit hexadecimal tal.

Så er det vel:

(16^3*5+16^2*14+16*13+4) - (0+7*16^2+10*16+4) = 22320_ten = 5730_16bithexa
Men man skal vist egentlig skrive det om til binær representation, så:

5ED4 = 0101 1110 1101 0100
07A4 = 0000 0111 1010 0100
Result 0101 0111 0011 0000

Exercise 3.3:

5ED4 = 0101 1110 1101 0100, as seen above
Hvorfor er base 16 (hexadecimal) attrctive numbering system for values in computers?
Nok fordi de kan indeholde meget information på lidt plads? Og har en nem 
konvertering til binær...

Exercise 3.20:

0 x 0C000000 repræsenterer hvad, hvis det er two's comlpement integer? Hvis det er unsigned integer?
Two's complement, der er første bit fortegnet, dvs. 0 er positivt (tror jeg?)

0X0C000000 = 
0000 1100 0000 0000 0000 0000 0000 0000

Mit problem er, jeg kan ikke se der er nogen forskel til unsigned og two's complement. 
Men det er der vist ikke, fordi det skulle ligge i den første bit (most significant).

Exercise 3.22:

Beregn hvad 0x0C000000 representerer som decimaltal, hvis det er et floating point number.

Vi har jo stadigvæk følgende binære oversættelse:
0X0C000000 = 0000 1100 0000 0000 0000 0000 0000 0000
For et floating point number skal det første tegn være fortegnet (som er 0, altså positivt). Så lidt 
nemmere at skrive sådan:

0X0C000000 = 0 0001 1000 - 0000 0000 0000 0000 0000 000
det første er fortegn, de næste 8 er eksponenten og de sidste 23 er "fraction".
Derfor fås at tallet i decimaltal er:
1*0*2^(24) = 0, i decimaltal. AHA! Men i IEEE 754 standard er formlen 1 + fraction, så derfor:

Eksponenten er 0001 1000 = 24_ten. Så skal vi 127 fra dette, vi får 24-127 = -103. Derfor:

1*(1+0)*2^(24-127) = 1.0*2^(-103).

Exercise 3.23:

Nu skal vi omregne 63.25 til binær representation via IEEE 743 single precision.
Først skal vi have normaliseret det:
x = 63.25 = 63 + 1/4 = 1*2^5+1*2^4+1*2^3+1*2^2+1*2^1+1*2^0+0*2^(-1)+1*2^(-2).
Vi skal skrive det i normaliseret scientific notation, så:
x = 1.1111101_two*2^5.
Nu skal vi lægge 127 til eksponenten:
5+127 = 132.
Det giver os følgende:
x = (-1)^0 * (1+.1111 1101 0000 0000 0000 0000_two)*2^(132)
Binær representation bliver så:
0 - 1000 0100 - 1111 1101 0000 0000 0000 0000

Exercise 3.24:

Nu skal vi gøre det samme som før, men antage en double precision, når vi omskriver.

Omskrivningen står ved magt, til gengæld er offsettet nu 1023 i stedet for 127. Derfor skal 
følgende eksponent udregnes:
5 + 1023 = 1028.
I binær 11-bit skrives dette:
10000000100_two = 1028_ten.
nu får vi så følgende binær reprsentation med double precision:

0 - 1000 0000 100 - 1111 1101 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000

Exercise 3.27:

Nu skal vi tage udgangspunkt i -1.5625*10^-1 i 16-bit notation., 5 steder til eksponent, 10 til fraction
(antaget foranstillet 1).
Vi ved at sign-bit skal være 1.
Vi skal prøve at omskrive på numerisk værdi af tallet:
-0.15625 = -10*1/64 = -10_ten / 2^6 = -1010_two / 2^6 = -0.001010_two

For at få det på korrekt form skrives:

-1.0100*2^(-3)

Det giver en eksponent på:

-3 + 15 = 12.

Så får vi skrevet tallet således på binær representation 16 bit:

1-01100-0100000000

Radix_sort-opgave:

lexicographical sorting means that e.g. 1111 > 0000, but if the binary is seen as a two's complement, this 
will obviously be wrong. How can we transform the bits of two's complement signed integers 
such that they can be compared lexicographically?

Hvis signed-bit is 1 in both numbers, the lexicographical sorting will give the reverse outcome of
what is intended (i.e. 1111 > 1000, however 1111 means very negative, so).
If signed-bit is 0 in both numbers, the sorting works as it should (of course).
If exactly one number has 1 in signed bit, then sorting again gives reverse
result, since the number with a 1 should be lowest.
What if we change all bits, if any one signed bit is 1? Then 1111 -> 0000 < 0111 <- 1000, so that works.
Also 1111 -> 0000 < 0100 <- 0100. I.e. if only one number has a signed bit, both numbers also change,
e.g. -128 < 4 --- (1111 1111 < 0000 0100) -> (0000 0000 < 1111 1011).
Aha! But also for the case of two 0's, we could easily just flip the first bit, since it does not 
change the lexicographical comparison! So solution is to flip the most significant bit.

part two: 
How can we transform the bits of IEEE 754 floating point numbers 
such that they can be compared lexicographically?

Well it should be the same, no? There is also a signed bit first, and it should work as above.
But wait! If the sign-bit was set before flipping, we also need to flip all other bits. This is because the
exponent is added with 127, so let's say one number is -2^0, another is -2^(1). Then one exponent
in IEEE 754 is 1111 1110, the other 1111 1111. However, the lowest number is of course -2^1, so we 
would want 1111 1111 < 1111 1110, and to make this happen we need to flip all bits (manually change the stuff)
of at least the exponent (the exponent determines size), and if these are equal then also the fractional part.